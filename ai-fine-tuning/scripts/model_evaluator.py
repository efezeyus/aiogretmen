#!/usr/bin/env python3
"""
Fine-tuned Model DeÄŸerlendirme Scripti
Model performansÄ±nÄ± kapsamlÄ± olarak test eder.
"""

import json
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import numpy as np
from openai import OpenAI
import argparse
from collections import Counter


class ModelEvaluator:
    """Model deÄŸerlendirme sÄ±nÄ±fÄ±"""
    
    def __init__(self, model_name: str, api_key: Optional[str] = None):
        self.model_name = model_name
        self.client = OpenAI(api_key=api_key)
        self.results_dir = Path('results/evaluation')
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    def load_test_cases(self, test_file: Path) -> List[Dict]:
        """Test vakalarÄ±nÄ± yÃ¼kle"""
        with open(test_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def evaluate_response(self, response: str, expected_criteria: Dict) -> Dict:
        """YanÄ±tÄ± deÄŸerlendir"""
        evaluation = {
            'accuracy': 0,
            'relevance': 0,
            'completeness': 0,
            'pedagogy': 0,
            'language_quality': 0,
            'grade_appropriateness': 0
        }
        
        # Basit skorlama (gerÃ§ek uygulamada daha sofistike olmalÄ±)
        
        # 1. DoÄŸruluk: Beklenen anahtar kelimeler var mÄ±?
        if 'keywords' in expected_criteria:
            found_keywords = sum(1 for kw in expected_criteria['keywords'] 
                               if kw.lower() in response.lower())
            evaluation['accuracy'] = found_keywords / len(expected_criteria['keywords'])
        
        # 2. Ä°lgililik: Konu dÄ±ÅŸÄ±na Ã§Ä±kÄ±lmÄ±ÅŸ mÄ±?
        evaluation['relevance'] = 0.8 if len(response) > 50 else 0.4
        
        # 3. TamlÄ±k: Yeterince detaylÄ± mÄ±?
        evaluation['completeness'] = min(len(response) / 500, 1.0)
        
        # 4. Pedagoji: Ã–ÄŸretici unsurlar var mÄ±?
        pedagogical_markers = ['Ã¶rnek', 'mesela', 'gibi', 'adÄ±m', 'kural', 'formÃ¼l']
        found_markers = sum(1 for marker in pedagogical_markers 
                          if marker in response.lower())
        evaluation['pedagogy'] = min(found_markers / 3, 1.0)
        
        # 5. Dil kalitesi: AnlaÅŸÄ±lÄ±r mÄ±?
        evaluation['language_quality'] = 0.85  # VarsayÄ±lan
        
        # 6. SÄ±nÄ±f seviyesine uygunluk
        if 'grade_level' in expected_criteria:
            grade_indicators = [f"{expected_criteria['grade_level']}. sÄ±nÄ±f", 
                              f"{expected_criteria['grade_level']} sÄ±nÄ±f"]
            evaluation['grade_appropriateness'] = 1.0 if any(ind in response.lower() 
                                                           for ind in grade_indicators) else 0.5
        
        # Genel skor
        evaluation['overall_score'] = np.mean(list(evaluation.values()))
        
        return evaluation
    
    def run_single_test(self, test_case: Dict) -> Dict:
        """Tek bir test vakasÄ±nÄ± Ã§alÄ±ÅŸtÄ±r"""
        start_time = time.time()
        
        try:
            # Model yanÄ±tÄ± al
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": test_case.get('system_prompt', 
                        "Sen MEB mÃ¼fredatÄ±na uygun eÄŸitim veren bir Ã¶ÄŸretmensin.")},
                    {"role": "user", "content": test_case['prompt']}
                ],
                temperature=0.7,
                max_tokens=500
            )
            
            answer = response.choices[0].message.content
            response_time = time.time() - start_time
            
            # YanÄ±tÄ± deÄŸerlendir
            evaluation = self.evaluate_response(answer, test_case.get('expected', {}))
            
            return {
                'success': True,
                'prompt': test_case['prompt'],
                'response': answer,
                'response_time': response_time,
                'evaluation': evaluation,
                'category': test_case.get('category', 'general'),
                'grade_level': test_case.get('expected', {}).get('grade_level')
            }
            
        except Exception as e:
            return {
                'success': False,
                'prompt': test_case['prompt'],
                'error': str(e),
                'response_time': time.time() - start_time,
                'category': test_case.get('category', 'general')
            }
    
    def run_evaluation_suite(self, test_cases: List[Dict]) -> Dict:
        """TÃ¼m test paketini Ã§alÄ±ÅŸtÄ±r"""
        print(f"\nğŸ§ª {len(test_cases)} test vakasÄ± Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        results = []
        categories_scores = {}
        grade_levels_scores = {}
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\râ³ Test {i}/{len(test_cases)}", end='', flush=True)
            
            result = self.run_single_test(test_case)
            results.append(result)
            
            # Kategori bazlÄ± skorlama
            if result['success']:
                category = result['category']
                if category not in categories_scores:
                    categories_scores[category] = []
                categories_scores[category].append(
                    result['evaluation']['overall_score']
                )
                
                # SÄ±nÄ±f seviyesi bazlÄ± skorlama
                grade = result.get('grade_level')
                if grade:
                    if grade not in grade_levels_scores:
                        grade_levels_scores[grade] = []
                    grade_levels_scores[grade].append(
                        result['evaluation']['overall_score']
                    )
        
        print("\nâœ… Testler tamamlandÄ±!")
        
        # Ã–zet istatistikler
        successful_tests = [r for r in results if r['success']]
        failed_tests = [r for r in results if not r['success']]
        
        summary = {
            'total_tests': len(results),
            'successful_tests': len(successful_tests),
            'failed_tests': len(failed_tests),
            'success_rate': len(successful_tests) / len(results) if results else 0,
            'avg_response_time': np.mean([r['response_time'] for r in successful_tests]) if successful_tests else 0,
            'category_scores': {cat: np.mean(scores) for cat, scores in categories_scores.items()},
            'grade_level_scores': {str(grade): np.mean(scores) for grade, scores in grade_levels_scores.items()},
            'detailed_results': results
        }
        
        # DetaylÄ± metrikler
        if successful_tests:
            all_evaluations = [r['evaluation'] for r in successful_tests]
            
            summary['metrics'] = {
                'accuracy': np.mean([e['accuracy'] for e in all_evaluations]),
                'relevance': np.mean([e['relevance'] for e in all_evaluations]),
                'completeness': np.mean([e['completeness'] for e in all_evaluations]),
                'pedagogy': np.mean([e['pedagogy'] for e in all_evaluations]),
                'language_quality': np.mean([e['language_quality'] for e in all_evaluations]),
                'grade_appropriateness': np.mean([e['grade_appropriateness'] for e in all_evaluations]),
                'overall': np.mean([e['overall_score'] for e in all_evaluations])
            }
        
        return summary
    
    def generate_report(self, evaluation_results: Dict) -> str:
        """DeÄŸerlendirme raporu oluÅŸtur"""
        report = []
        report.append("=" * 70)
        report.append(f"MODEL DEÄERLENDÄ°RME RAPORU")
        report.append(f"Model: {self.model_name}")
        report.append(f"Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("=" * 70)
        
        # Genel Ã¶zet
        report.append("\nğŸ“Š GENEL Ã–ZET:")
        report.append(f"  â€¢ Toplam test: {evaluation_results['total_tests']}")
        report.append(f"  â€¢ BaÅŸarÄ±lÄ±: {evaluation_results['successful_tests']}")
        report.append(f"  â€¢ BaÅŸarÄ±sÄ±z: {evaluation_results['failed_tests']}")
        report.append(f"  â€¢ BaÅŸarÄ± oranÄ±: {evaluation_results['success_rate']:.1%}")
        report.append(f"  â€¢ Ortalama yanÄ±t sÃ¼resi: {evaluation_results['avg_response_time']:.2f}s")
        
        # Metrik skorlarÄ±
        if 'metrics' in evaluation_results:
            report.append("\nğŸ“ˆ METRÄ°K SKORLARI (0-1):")
            for metric, score in evaluation_results['metrics'].items():
                report.append(f"  â€¢ {metric.title()}: {score:.3f}")
        
        # Kategori bazlÄ± performans
        if evaluation_results['category_scores']:
            report.append("\nğŸ“š KATEGORÄ° BAZLI PERFORMANS:")
            for category, score in sorted(evaluation_results['category_scores'].items()):
                report.append(f"  â€¢ {category}: {score:.3f}")
        
        # SÄ±nÄ±f seviyesi bazlÄ± performans
        if evaluation_results['grade_level_scores']:
            report.append("\nğŸ“ SINIF SEVÄ°YESÄ° BAZLI PERFORMANS:")
            for grade, score in sorted(evaluation_results['grade_level_scores'].items()):
                report.append(f"  â€¢ {grade}. sÄ±nÄ±f: {score:.3f}")
        
        # BaÅŸarÄ±sÄ±z testler
        failed = [r for r in evaluation_results['detailed_results'] if not r['success']]
        if failed:
            report.append(f"\nâŒ BAÅARISIZ TESTLER ({len(failed)}):")
            for fail in failed[:5]:
                report.append(f"  â€¢ Soru: {fail['prompt'][:50]}...")
                report.append(f"    Hata: {fail['error']}")
        
        # En iyi ve en kÃ¶tÃ¼ yanÄ±tlar
        successful = [r for r in evaluation_results['detailed_results'] if r['success']]
        if successful:
            sorted_by_score = sorted(successful, 
                                   key=lambda x: x['evaluation']['overall_score'], 
                                   reverse=True)
            
            report.append("\nâœ… EN Ä°YÄ° YANITLAR:")
            for result in sorted_by_score[:3]:
                report.append(f"  â€¢ Soru: {result['prompt']}")
                report.append(f"    Skor: {result['evaluation']['overall_score']:.3f}")
                report.append(f"    YanÄ±t: {result['response'][:100]}...")
            
            report.append("\nâš ï¸  EN KÃ–TÃœ YANITLAR:")
            for result in sorted_by_score[-3:]:
                report.append(f"  â€¢ Soru: {result['prompt']}")
                report.append(f"    Skor: {result['evaluation']['overall_score']:.3f}")
                report.append(f"    YanÄ±t: {result['response'][:100]}...")
        
        report_text = "\n".join(report)
        
        # Raporu kaydet
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.results_dir / f"evaluation_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        # JSON sonuÃ§larÄ± da kaydet
        results_file = self.results_dir / f"evaluation_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ Rapor kaydedildi: {report_file}")
        print(f"ğŸ“ DetaylÄ± sonuÃ§lar: {results_file}")
        
        return report_text


def create_default_test_cases() -> List[Dict]:
    """VarsayÄ±lan test vakalarÄ± oluÅŸtur"""
    return [
        # Matematik testleri
        {
            "prompt": "5. sÄ±nÄ±f Ã¶ÄŸrencisiyim. Kesirler konusunu anlamadÄ±m.",
            "category": "matematik",
            "expected": {
                "keywords": ["pay", "payda", "bÃ¼tÃ¼n", "parÃ§a", "kesir"],
                "grade_level": 5
            }
        },
        {
            "prompt": "3/4 + 1/4 iÅŸlemini nasÄ±l yaparÄ±m?",
            "category": "matematik",
            "expected": {
                "keywords": ["payda", "aynÄ±", "topla", "4/4", "1"],
                "grade_level": 5
            }
        },
        {
            "prompt": "8. sÄ±nÄ±f Pisagor teoremi nedir?",
            "category": "matematik",
            "expected": {
                "keywords": ["dik Ã¼Ã§gen", "hipotenÃ¼s", "kare", "aÂ²+bÂ²=cÂ²"],
                "grade_level": 8
            }
        },
        
        # TÃ¼rkÃ§e testleri
        {
            "prompt": "Ä°sim ve fiil arasÄ±ndaki fark nedir?",
            "category": "turkce",
            "expected": {
                "keywords": ["varlÄ±k", "hareket", "eylem", "ad"],
                "grade_level": 5
            }
        },
        {
            "prompt": "Noktalama iÅŸaretlerini nasÄ±l kullanÄ±rÄ±m?",
            "category": "turkce",
            "expected": {
                "keywords": ["nokta", "virgÃ¼l", "soru", "Ã¼nlem"],
                "grade_level": 4
            }
        },
        
        # Fen Bilimleri testleri
        {
            "prompt": "6. sÄ±nÄ±f vÃ¼cudumuzda sistemler konusu",
            "category": "fen",
            "expected": {
                "keywords": ["iskelet", "kas", "sindirim", "dolaÅŸÄ±m", "solunum"],
                "grade_level": 6
            }
        },
        {
            "prompt": "9. sÄ±nÄ±f hÃ¼cre nedir?",
            "category": "fen",
            "expected": {
                "keywords": ["canlÄ±", "yapÄ±", "Ã§ekirdek", "sitoplazma", "zar"],
                "grade_level": 9
            }
        },
        
        # Ã‡ok sÄ±nÄ±flÄ± testler
        {
            "prompt": "Ben 2. sÄ±nÄ±f Ã¶ÄŸrencisiyim. Saat okumayÄ± Ã¶ÄŸret.",
            "category": "matematik",
            "expected": {
                "keywords": ["akrep", "yelkovan", "saat", "dakika"],
                "grade_level": 2
            }
        },
        {
            "prompt": "11. sÄ±nÄ±f tÃ¼rev konusu",
            "category": "matematik",
            "expected": {
                "keywords": ["deÄŸiÅŸim", "hÄ±z", "limit", "f'(x)"],
                "grade_level": 11
            }
        }
    ]


def main():
    parser = argparse.ArgumentParser(description='Fine-tuned model deÄŸerlendirme')
    parser.add_argument('model', help='DeÄŸerlendirilecek model adÄ±')
    parser.add_argument('--test-file', type=Path, 
                       help='Test vakalarÄ± JSON dosyasÄ±')
    parser.add_argument('--quick', action='store_true',
                       help='HÄ±zlÄ± test (sadece 5 vaka)')
    
    args = parser.parse_args()
    
    # Test vakalarÄ±nÄ± yÃ¼kle
    if args.test_file and args.test_file.exists():
        test_cases = json.load(open(args.test_file, 'r', encoding='utf-8'))
    else:
        test_cases = create_default_test_cases()
    
    # HÄ±zlÄ± test modu
    if args.quick:
        test_cases = test_cases[:5]
    
    # DeÄŸerlendirici oluÅŸtur
    evaluator = ModelEvaluator(args.model)
    
    # DeÄŸerlendirme yap
    results = evaluator.run_evaluation_suite(test_cases)
    
    # Rapor oluÅŸtur
    report = evaluator.generate_report(results)
    
    # Raporu ekrana yazdÄ±r
    print("\n" + report)


if __name__ == '__main__':
    main() 